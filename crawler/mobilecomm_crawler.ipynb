{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd044b5a777b7624e45e2527e4751e65d23935e9f9a280750eed838ca082febb7b4",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from google.cloud import language_v1\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from crawler_model import Ptt\n",
    "\n",
    "ua = UserAgent()\n",
    "fakeua = ua.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "with open('phone_catalog.json', 'r') as fp:\n",
    "    phone_dict = json.load(fp)"
   ]
  },
  {
   "source": [
    "# phone_list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phones():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_Android_smartphones\"\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "    rows = []\n",
    "    for i in tables:\n",
    "        rows.extend(i.select(\"tbody tr th\"))\n",
    "\n",
    "    phones = [a.text.partition(\"(\")[0].strip() for a in rows]\n",
    "    phones = [ p for p in phones if len(p.split()) > 1]\n",
    "\n",
    "    brands = set()\n",
    "    for p in phones:\n",
    "        brands.add(p.split(\" \")[0])\n",
    "    brands = list(brands)\n",
    "    brands = sorted(brands)\n",
    "    for b in range(len(brands)):\n",
    "        if brands[b] == \"Asus\":\n",
    "            brands = brands[b:]\n",
    "            break\n",
    "\n",
    "    phone_list = []\n",
    "    for b in brands:\n",
    "        if b == \"Samsung\":\n",
    "            for p in phones:\n",
    "                if p.split(\" \")[0] == b:\n",
    "                    if \"/\" in p:\n",
    "                        p_lst = p.split(\"/\")\n",
    "                        for series in p_lst[1:]:\n",
    "                            sub_p = p_lst[0].replace(\"5G\", \"\").strip()\n",
    "                            if series == \"+\":\n",
    "                                phone = sub_p + series\n",
    "                            else:\n",
    "                                phone = sub_p + \" \" + series\n",
    "                            if len(phone) > 0:\n",
    "                                phone_list.append(phone)\n",
    "                        phone_list.append(sub_p)\n",
    "                    else:\n",
    "                        phone = p.replace(\"5G\", \"\").strip()\n",
    "                        if len(phone) > 0:\n",
    "                            phone_list.append(phone)\n",
    "        elif b == \"POCO\":\n",
    "            for p in phones:\n",
    "                if p.split(\" \")[0] == b:\n",
    "                    phone = p.replace(\"5G\", \"\").strip()\n",
    "                    if len(phone) > 0:\n",
    "                        phone_list.append(phone)\n",
    "\n",
    "        elif b == \"Pixel\":\n",
    "            for p in phones:\n",
    "                if p.split(\" \")[0] == b:\n",
    "                    phone = p.replace(\"5G\", \"\").strip()\n",
    "                    if len(phone) > 0:\n",
    "                        phone_list.append(phone)\n",
    "        else:\n",
    "            for p in phones:\n",
    "                if p.split(\" \")[0] == b:\n",
    "                    if \"/\" in p:\n",
    "                        p_lst = p.split(\"/\")\n",
    "                        for series in p_lst[1:]:\n",
    "                            sub_p = p_lst[0].replace(\"5G\", \"\").strip()\n",
    "                            if series == \"+\":\n",
    "                                phone = sub_p + series\n",
    "                            else:\n",
    "                                phone = sub_p + \" \" + series\n",
    "                            if len(phone) > 0:\n",
    "                                phone_list.append(phone)\n",
    "                        phone_list.append(sub_p)\n",
    "                    else:\n",
    "                        phone = p.replace(\"5G\", \"\").strip()\n",
    "                        if len(phone) > 0:\n",
    "                            phone_list.append(phone)\n",
    "                        phone_list.append(phone)\n",
    "    phone_list.append(\"Asus ROG Phone 2\")\n",
    "    extend = [\"note20ultra\", \"s20ultra\", \"s21ultra\", \"s20+\", \"s21+\", \"s20\", \"s21\"]\n",
    "    for s in extend:\n",
    "        phone_list.append(s)\n",
    "\n",
    "    url = \"https://www.theiphonewiki.com/wiki/List_of_iPhones\"\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"div\", id=\"toc\")\n",
    "    iphones = []\n",
    "    for span in table.select(\"ul li span\"):\n",
    "        if \"iPhone\" in str(span):\n",
    "            if (\"SE\" in str(span)) and (\"1\" in str(span)):\n",
    "                iphones.append(\"iPhone SE\")\n",
    "            elif (\"SE\" in str(span)) and (\"2\" in str(span)):\n",
    "                iphones.append(\"iPhone SE2\")\n",
    "            else:\n",
    "                iphones.append(span.text)\n",
    "    iphones.remove(\"iPhone\")\n",
    "    phone_list.extend(iphones)\n",
    "    phone_set = set()\n",
    "    for p in phone_list:\n",
    "        phone_set.add(p)\n",
    "    phone_list = sorted(list(phone_set))\n",
    "    phone_list.remove(\"Asus ROG Phone II\")\n",
    "    return phone_list"
   ]
  },
  {
   "source": [
    "# crawl PTT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_page(url):\n",
    "    ua = UserAgent()\n",
    "    fakeua = ua.random\n",
    "    headers = {\"Origin\": \"https://www.ptt.cc\",\n",
    "            \"Referer\": \"https://fonts.googleapis.com/\",\n",
    "            \"sec-ch-ua-mobile\": \"?0\",\n",
    "            \"User-Agent\": fakeua}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    previous = soup.find_all(\"a\", class_=\"btn wide\")[0][\"href\"]\n",
    "    previous_page_num = int(re.findall(r\"\\d+\", previous)[0])\n",
    "    return previous_page_num\n",
    "last_page = get_last_page(\"https://www.ptt.cc/bbs/MobileComm/search?page=1&q=%E5%BF%83%E5%BE%97\")\n",
    "last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_analyze_sentiment(text_content):\n",
    "    \"\"\"\n",
    "    Analyzing Sentiment in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= os.path.join(os.path.expanduser(\"~\"), \"smartphone-smartprice-key.json\")\n",
    "\n",
    "    post = {}\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # text_content = 'I am so happy and joyful.'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    document = {\"content\": text_content, \"type_\": type_}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(request = {'document': document, 'encoding_type': encoding_type})\n",
    "    # Get overall sentiment of the input document\n",
    "    doc = {}\n",
    "    doc[\"score\"] = response.document_sentiment.score\n",
    "    \n",
    "    doc[\"magnitude\"] = response.document_sentiment.magnitude\n",
    "\n",
    "    # Get sentiment for all sentences in the document\n",
    "    sentences = []\n",
    "    for sentence in response.sentences:\n",
    "        # if abs(sentence.sentiment.score) >= 0.5 and sentence.sentiment.magnitude >= 0.5:\n",
    "        d = {}\n",
    "        d[\"content\"] = sentence.text.content.replace(\".\", \"\").replace(\"。\", \"\")\n",
    "        d[\"score\"] = sentence.sentiment.score\n",
    "        d[\"magnitude\"] = sentence.sentiment.magnitude\n",
    "        sentences.append(d)\n",
    "    \n",
    "    return doc, sentences\n",
    "\n",
    "def crawl_comm(links):\n",
    "    for link in links:\n",
    "        url = \"https://www.ptt.cc\" + link[1]\n",
    "        # GET request from url and parse via BeautifulSoup\n",
    "        ua = UserAgent()\n",
    "        fakeua = ua.random\n",
    "        headers = {\"Origin\": \"https://www.ptt.cc\",\n",
    "                \"Referer\": \"https://fonts.googleapis.com/\",\n",
    "                \"sec-ch-ua-mobile\": \"?0\",\n",
    "                \"User-Agent\": fakeua}\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        if \"200\" in str(resp):\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            main = soup.find(\"div\", id=\"main-content\")\n",
    "            # keywords = [\"優點\", \"缺點\", \"照相\", \"電池\", \"續航\"]\n",
    "            if \"※ 引述\" in str(main):   \n",
    "                body = str(main).split('※ 引述')[0].split('</span></div>')[-1].replace(\" \",\"\").replace(\"\\n\", \"。\")\n",
    "            else:\n",
    "                body = str(main).split('--')[0].split('</span></div>')[-1].replace(\" \",\"\").replace(\"\\n\", \"。\")\n",
    "        doc, sentences = sample_analyze_sentiment(body)\n",
    "        d = {\"title\": link[0], \"link\": link[1], \"doc\": doc, \"sentences\": sentences}\n",
    "        print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "data = []\n",
    "error_links = []\n",
    "for page in range(1, ):\n",
    "\n",
    "    ua = UserAgent()\n",
    "    fakeua = ua.random\n",
    "    headers = {\"Origin\": \"https://www.ptt.cc\",\n",
    "            \"Referer\": \"https://fonts.googleapis.com/\",\n",
    "            \"sec-ch-ua-mobile\": \"?0\",\n",
    "            \"User-Agent\": fakeua}\n",
    "\n",
    "    url = \"https://www.ptt.cc/bbs/MobileComm/search?page={}&q=%E5%BF%83%E5%BE%97\".format(page)\n",
    "    print(url)\n",
    "    phone_list = get_phones()\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    index_soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    keywords = [\"換\", \"vs\", \"v.s\", \"vs.\", \"v.s.\"]\n",
    "    title = index_soup.findAll(\"div\", class_=\"title\")\n",
    "\n",
    "    links = []\n",
    "    for t in title:\n",
    "        if any(keyword in t.text for keyword in keywords):\n",
    "            pass\n",
    "        else:\n",
    "            for p in phone_list:\n",
    "                if p.replace(\" \", \"\").lower() in t.text.replace(\" \", \"\").lower():\n",
    "                    print(p, t)\n",
    "                    links.append( (t, t.find(\"a\")[\"href\"]) )\n",
    "                    break\n",
    "                    break\n",
    "        \n",
    "\n",
    "    crawl_comm(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(len(data))\n",
    "data[0]"
   ]
  },
  {
   "source": [
    "# Sentiment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentifish import Sentiment\n",
    "text='''2020/02自美國雅馬遜購入\n",
    "昨天發現電池膨脹了\n",
    "爬了一下板上資訊\n",
    "應該可以更換保固區域後以延長保固1年為由\n",
    "更換（整）新機\n",
    "\n",
    "但也看到線上服務看得是一種運\n",
    "不是大好就是大壞\n",
    "不巧小弟抽到壞的那邊\n",
    "客服堅持不是官網買的無法處理\n",
    "也沒有換保固區域的處理\n",
    "另外也說電池膨脹不在延長保固之範圍\n",
    "（但電量消耗大是可以的）\n",
    "\n",
    "總之就是叫我自己去找雅馬遜處理\n",
    "\n",
    "我想這樣的客服\n",
    "我大概不會再買pixel了吧'''\n",
    "obj=Sentiment(text)\n",
    "polarity = obj.analyze( )\n",
    "polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1195a794feb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://shopee.tw/product/354450050/8754002931\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprettfy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'user-agent': fakeua,\n",
    "    'x-api-source': 'pc',\n",
    "    'referer': \"https://shopee.tw/product/354450050/8754002931\"\n",
    "    }\n",
    "r = requests.get(\"https://shopee.tw/product/354450050/8754002931\", headers=headers)\n",
    "print(r)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "print(soup.prettfy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}